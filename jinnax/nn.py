#Functions to train NN
import jax
import jax.numpy as jnp
import optax
from alive_progress import alive_bar
import math
import time
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pickle

__docformat__ = "numpy"

#MSE
@jax.jit
def MSE(pred,true):
    """
    Mean square error
    ----------

    Parameters
    ----------
    pred : jax.numpy.array

        A JAX numpy array with the predicted values

    true : jax.numpy.array

        A JAX numpy array with the true values

    Returns
    -------
    mean square error
    """
    return jnp.mean((true - pred)**2)

#L2 error
@jax.jit
def L2error(pred,true):
    """
    L2-error
    ----------

    Parameters
    ----------
    pred : jax.numpy.array

        A JAX numpy array with the predicted values

    true : jax.numpy.array

        A JAX numpy array with the true values

    Returns
    -------
    L2-error
    """
    return jnp.sqrt(jnp.sum((true - pred)**2))/jnp.sqrt(jnp.sum(true ** 2))

#Simple fully connected architecture. Return the initial parameters and the function for the forward pass
def fconNN(width,activation = jax.nn.tanh,key = 0):
    """
    Initialize fully connected neural network
    ----------

    Parameters
    ----------
    width : list

        List with the layers width

    activation : jax.nn activation

        The activation function. Default jax.nn.tanh

    key : int

        Seed for parameters initialization. Default 0

    Returns
    -------
    dict with initial parameters and the function for the forward pass
    """
    #Initialize parameters with Glorot initialization
    initializer = jax.nn.initializers.glorot_normal()
    key = jax.random.split(jax.random.PRNGKey(key),len(width)-1) #Seed for initialization
    params = list()
    for key,lin,lout in zip(key,width[:-1],width[1:]):
        W = initializer(key,(lin,lout),jnp.float32)
        B = initializer(key,(1,lout),jnp.float32)
        params.append({'W':W,'B':B})

    #Define function for forward pass
    @jax.jit
    def forward(x,params):
      *hidden,output = params
      for layer in hidden:
        x = activation(x @ layer['W'] + layer['B'])
      return x @ output['W'] + output['B']

    #Return initial parameters and forward function
    return {'params': params,'forward': forward}

#Training PINN
def train_PIIN(data,width,pde,test_data = None,epochs = 100,activation = jax.nn.tanh,lr = 0.001,b1 = 0.9,b2 = 0.999,eps = 1e-08,eps_root = 0.0,key = 0,epoch_print = 100,plot = False,times = 5,d2 = False,save = False,file_name = 'result_pinn'):
    """
    Train a Physics-informed Neural Network
    ----------

    Parameters
    ----------
    data : dict

        Data generated by the jinnax.data.generate_PINNdata function

    width : list

        A list with the width of each layer

    pde : function

        The partial differential operator. Its arguments are u, x and t

    test_data : dict, None

        A dictionay with test data for L2 error calculation generated by the jinnax.data.generate_PINNdata function. Default None for not calculating L2 error

    epochs : int

        Number of training epochs. Default 100

    activation : jax.nn activation

        The activation function of the neural network. Default jax.nn.tanh

    lr,b1,b2,eps,eps_root: float

        Hyperparameters of the Adam algorithm. Default lr = 0.001, b1 = 0.9, b2 = 0.999, eps = 1e-08, eps_root = 0.0

    key : int

        Seed for parameters initialization. Default 0

    epoch_print : int

        Number of epochs to calculate, save and print test error, and display and save plots. Default 100

    plot : logical

        Whether to plot the results from time to time when the spatial dimension is one. Default False

    times : int

        Number of points along the time interval to plot. Default 5

    d2 : logical

        Whether to plot 2D plot when the spatial dimension is one. Default False

    save : logical

        Whether to save the plots, the L2 error and the current parameters. Default False

    file_name : str

        File prefix to save the plots, the L2 error and the current parameters. Default 'result_pinn'

    Returns
    -------
    dict-like object with the estimated function, the estimated parameters and the neural network function for the forward pass
    """

    #Initialize architecture
    nnet = jar.fconNN(width,activation,key)
    forward = nnet['forward']
    params = nnet['params']

    #Define loss function
    @jax.jit
    def lf(params,x):
        loss = 0
        if x['sensor'] is not None:
            #Term that refers to sensor data
            loss = loss + jnp.mean(jax.vmap(MSE,in_axes = (0,0))(forward(x['sensor'],params),x['usensor']))
        if x['boundary'] is not None:
            #Term that refers to boundary data
            loss = loss + jnp.mean(jax.vmap(MSE,in_axes = (0,0))(forward(x['boundary'],params),x['uboundary']))
        if x['initial'] is not None:
            #Term that refers to initial data
            loss = loss + jnp.mean(jax.vmap(MSE,in_axes = (0,0))(forward(x['initial'],params),x['uinitial']))
        if x['collocation'] is not None:
            #Term that refers to collocation points
            x_col = x['collocation'][:,:-1].reshape((x['collocation'].shape[0],x['collocation'].shape[1] - 1))
            t_col = x['collocation'][:,-1].reshape((x['collocation'].shape[0],1))
            loss = loss + MSE(pde(lambda x,t: forward(jnp.append(x,t,1),params),x_col,t_col),0)
        return loss

    #Initialize Adama oOptmizer
    optimizer = optax.adam(lr,b1,b2,eps,eps_root)
    opt_state = optimizer.init(params)

    #Define the gradient function
    grad_loss = jax.jit(jax.grad(lf,0))

    #Define update function
    @jax.jit
    def update(opt_state,params,x):
        #Compute gradient
        grads = grad_loss(params,x)
        #Calculate parameters updates
        updates, opt_state = optimizer.update(grads, opt_state)
        #Update parameters
        params = optax.apply_updates(params, updates)
        #Return state of optmizer and updated parameters
        return opt_state,params

    ###Training###
    t0 = time.time()
    #Initialize alive_bar for tracing in terminal
    with alive_bar(epochs) as bar:
        #For each epoch
        for e in range(epochs):
            #Update optimizer state and parameters
            opt_state,params = update(opt_state,params,data)
            #After epoch_print epochs
            if e % epoch_print == 0:
                #Compute elapsed time and current error
                l = 'Time: ' + str(round(time.time() - t0)) + ' s Loss: ' + str(jnp.round(lf(params,data),6))
                #If there is test data, compute current L2 error
                if test_data is not None:
                    #Compute L2 error and build plots
                    res = process_result(test_data,lambda xt: forward(xt,params),data,plot = plot,times = times,d2 = d2,save = save,file_name = file_name + '_epoch' + str(e).rjust(6, '0'),print = False)
                    l = l + ' L2 error: ' + str(jnp.round(res['l2_error'][0],6))
                #Print
                print(l)
                if save:
                    #Save current parameters
                    pickle.dump({'params': params,'width': width},open(file_name + '_epoch' + str(e).rjust(6, '0') + '.pickle','wb'), protocol = pickle.HIGHEST_PROTOCOL)
            #Update alive_bar
            bar()
    #Define estimated function
    def u(xt):
        return forward(xt,params)

    return {'u': u,'params': params,'forward': forward}

#Process result
def process_result(test_data,u_trained,train_data,plot = True,times = 5,d2 = True,save = False,file_name = 'result_pinn',print = True):
    """
    Process the results of a Physics-informed Neural Network
    ----------

    Parameters
    ----------
    test_data : dict

        A dictionay with test data for L2 error calculation generated by the jinnax.data.generate_PINNdata function

    u_trained : function

        Function estimated by a PINN

    train_data : dict

        Training data generated by the jinnax.data.generate_PINNdata

    plot : logical

        Wheter to generate plots comparing the exact and estimated solutions when the spatial dimension is one. Default True

    times : int

        Number of points along the time interval to plot. Default 5

    d2 : logical

        Whether to plot 2D plot when the spatial dimension is one. Default True

    save : logical

        Whether to save the plots and the L2 error. Default False

    file_name : str

        File prefix to save the plots and the L2 error. Default 'result_pinn'

    print : logical

        Wheter to print the L2 error. Default True

    Returns
    -------
    pandas data frame with L2 error
    """

    #Dimension
    d = test_data['xt'].shape[1] - 1

    #Number of plots multiple of 5
    times = 5 * round(times/5.0)

    #Data
    xt = test_data['xt']
    u = test_data['u']
    upred = u_trained(xt)

    #Results
    l2_error = L2error(upred,u)
    sensor_sample = train_data['sensor'].shape[0]
    boundary_sample = train_data['boundary'].shape[0]
    initial_sample = train_data['initial'].shape[0]
    collocation_sample = train_data['collocation'].shape[0]
    df = pd.DataFrame(np.array([sensor_sample,boundary_sample,initial_sample,collocation_sample,l2_error.tolist()]).reshape((1,5)), columns=['sensor_sample','boundary_sample','initial_sample','collocation_sample','l2_error'])
    if save:
        df.to_csv(file_name + '.csv',index = False)
    if print:
        print('L2 error: ' + str(jnp.round(l2_error,6)))

    #Plots
    if d == 1:
        fig, ax = plt.subplots(int(times/5),5)
        fig.tight_layout()
        tlo = jnp.min(xt[:,-1])
        tup = jnp.max(xt[:,-1])
        ylo = jnp.min(jnp.append(u,upred,0))
        yup = jnp.max(jnp.append(u,upred,0))
        k = 0
        t_values = np.linspace(tlo,tup,times)
        for i in range(int(times/5)):
            for j in range(5):
                if k < len(t_values):
                    t = t_values[k]
                    t = xt[jnp.abs(xt[:,-1] - t) == jnp.min(jnp.abs(xt[:,-1] - t)),-1][0].tolist()
                    x_plot = xt[xt[:,-1] == t,:-1]
                    y_plot = upred[xt[:,-1] == t,:]
                    u_plot = u[xt[:,-1] == t,:]
                    if int(times/5) > 1:
                        ax[i,j].plot(x_plot[:,0],u_plot[:,0],'b-',linewidth=2,label='Exact')
                        ax[i,j].plot(x_plot[:,0],y_plot,'r--',linewidth=2,label='Prediction')
                        ax[i,j].set_title('$t = %.2f$' % (t),fontsize=10)
                        ax[i,j].set_xlabel('$x$')
                        ax[i,j].set_ylim([1.3 * ylo.tolist(),1.3 * yup.tolist()])
                    else:
                        ax[j].plot(x_plot[:,0],u_plot[:,0],'b-',linewidth=2,label='Exact')
                        ax[j].plot(x_plot[:,0],y_plot,'r--',linewidth=2,label='Prediction')
                        ax[j].set_title('$t = %.2f$' % (t),fontsize=10)
                        ax[j].set_xlabel('$x$')
                        ax[j].set_ylim([1.3 * ylo.tolist(),1.3 * yup.tolist()])
                    k = k + 1


        fig = plt.gcf()
        if plot:
            plt.show()
        if save:
            fig.savefig(file_name + '_slices.png')
        plt.close()

        #2d plot
        if d2:
            fig, ax = plt.subplots(1,2)
            l = int(jnp.sqrt(xt.shape[0]).tolist())
            ax[0].pcolormesh(xt[:,-1].reshape((l,l)),xt[:,0].reshape((l,l)),u[:,0].reshape((l,l)),cmap = 'RdBu',vmin = ylo.tolist(),vmax = yup.tolist())
            ax[0].set_title('Exact')
            ax[1].pcolormesh(xt[:,-1].reshape((l,l)),xt[:,0].reshape((l,l)),upred[:,0].reshape((l,l)),cmap = 'RdBu',vmin = ylo.tolist(),vmax = yup.tolist())
            ax[1].set_title('Predicted')

            fig = plt.gcf()
            if plot:
                plt.show()
            if save:
                fig.savefig(file_name + '_2d.png')
            plt.close()

    return df
